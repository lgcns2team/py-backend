import json
import logging
import os
import boto3
import requests
from uuid import UUID
from contextlib import closing

from django.http import StreamingHttpResponse, JsonResponse
from django.views.decorators.http import require_http_methods
from django.views.decorators.csrf import csrf_exempt

# Bedrock ê´€ë ¨ ê³µí†µ ëª¨ë“ˆ
from common.bedrock.clients import BedrockClients
from common.bedrock.streaming import sse_event

# API ë¬¸ì„œí™” ë° REST í”„ë ˆì„ì›Œí¬ ê´€ë ¨
from drf_spectacular.utils import extend_schema, OpenApiTypes
from rest_framework.decorators import api_view
from rest_framework import serializers

# ëª¨ë¸ ë° ë ˆí¬ì§€í† ë¦¬
from apps.prompt.models import AIPerson
from apps.prompt.redis_chat_repository import RedisChatRepository
from apps.prompt.dto import MessageDTO

logger = logging.getLogger(__name__)

from apps.prompt.models import AIPerson

from dotenv import load_dotenv
load_dotenv()

@csrf_exempt
@require_http_methods(["POST"])
def prompt_view(request, promptId=None):
    """Bedrock Prompt í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°) - FastAPI ë¡œì§ í¬íŒ…"""
    env_prompt_arn = os.getenv('AWS_BEDROCK_AI_PERSON_ARN')
    
    try:
        data = json.loads(request.body)
        
        # promptIdëŠ” URLì—ì„œ, user_queryëŠ” bodyì—ì„œ
        prompt_id = promptId or data.get('prompt_id')
        user_query = data.get('message') or data.get('user_query')
        
        user_id = request.GET.get("userId") or data.get("userId")

        if not prompt_id or not user_query:
            return StreamingHttpResponse(
                [sse_event({'type': 'error', 'message': 'Missing prompt_id or message'})],
                content_type='text/event-stream'
            )
        
        if not user_id:
            return StreamingHttpResponse(
                [sse_event({"type": "error", "message": "Missing userId in query param"})],
                content_type="text/event-stream",
            )
        
        try:
            user_id = UUID(user_id)
        except ValueError:
            return StreamingHttpResponse(
                [sse_event({'type': 'error', 'message': 'Invalid userId'})],
                content_type='text/event-stream'
            )

        redis_repo = RedisChatRepository()
        history_key = redis_repo.build_aiperson_key(prompt_id, user_id)

        user_msg = MessageDTO.user(user_query)

        def on_done_save(full_response: str):
            try:
                assistant_msg = MessageDTO.assistant(full_response)
                redis_repo.append_message(history_key, user_msg)
                redis_repo.append_message(history_key, assistant_msg)
                logger.info("Saved chat history key=%s (user_len=%s, assistant_len=%s)",
                            history_key, len(user_query), len(full_response))
            except Exception as e:
                logger.error("Redis save failed: %s", str(e))
    
        try:
            ai_person = AIPerson.objects.get(promptId=prompt_id)
            logger.info(f"Found AI Person: {ai_person.name} from {ai_person.era}")

            person_variables = {
                'name': ai_person.name,
                'era': ai_person.era,
                'summary': ai_person.summary or '',
                'year': str(ai_person.year) if ai_person.year else '',
                'greeting_message': ai_person.greetingMessage or '',
                'ex_question': ai_person.exQuestion or '',
            }

            if ai_person.latitude is not None and ai_person.longitude is not None:
                person_variables['location'] = f"ìœ„ë„: {ai_person.latitude}, ê²½ë„: {ai_person.longitude}"

        except AIPerson.DoesNotExist:
            logger.warning(f"AI Person not found for prompt_id: {prompt_id}")
            person_variables = {}

        variables = data.get('variables', {})
        prompt_variables = {
            "user_query": user_query,
            **person_variables,  # AI ì¸ë¬¼ ì •ë³´
            **variables
        }

        logger.info(f"Prompt variables: {list(prompt_variables.keys())}")

        # Bedrock Agent í´ë¼ì´ì–¸íŠ¸
        bedrock_agent = boto3.client(
            service_name='bedrock-agent',
            region_name=os.getenv('CLOUD_AWS_REGION', 'ap-northeast-2')
        )
        
        if prompt_id and prompt_id.startswith('arn:'):
            prompt_identifier = prompt_id
        else:
            prompt_identifier = env_prompt_arn

        if not prompt_identifier:
            logger.error("ì—ëŸ¬: í™˜ê²½ë³€ìˆ˜ AWS_BEDROCK_AI_PERSONì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        logger.info(f"Using Prompt ARN: {prompt_identifier}")
        
        try:
            # Prompt ê°€ì ¸ì˜¤ê¸°
            prompt_response = bedrock_agent.get_prompt(
                promptIdentifier=prompt_identifier
            )
            
            logger.info(f"Prompt retrieved: {prompt_response.get('name', 'Unknown')}")
            
            variants = prompt_response.get('variants', [])
            if not variants:
                raise ValueError("Prompt has no variants")
            
            variant = variants[0]
            template_type = variant.get('templateType', 'TEXT')
            
            logger.info(f"Template type: {template_type}")
            
            model_id = prompt_response.get('defaultModelId', 'anthropic.claude-3-5-sonnet-20240620-v1:0')
            
            # Bedrock Runtime
            bedrock_runtime = BedrockClients.get_runtime()
            
            # TEXT í…œí”Œë¦¿ ì²˜ë¦¬
            if template_type == 'TEXT':
                template_config = variant.get('templateConfiguration', {})
                template_text = template_config.get('text', {}).get('text', '')
                
                # ë³€ìˆ˜ ì¹˜í™˜
                formatted_prompt = template_text
                for var_name, var_value in prompt_variables.items():
                    formatted_prompt = formatted_prompt.replace(f"{{{{{var_name}}}}}", str(var_value))
                
                logger.info(f"Formatted prompt (first 100 chars): {formatted_prompt[:100]}...")
                
                inference_config = variant.get('inferenceConfiguration', {})
                
                body = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": inference_config.get('maxTokens', 4096),
                    "temperature": inference_config.get('temperature', 1.0),
                    "messages": [
                        {
                            "role": "user",
                            "content": formatted_prompt
                        }
                    ]
                }
                
                if 'stopSequences' in inference_config:
                    body['stop_sequences'] = inference_config['stopSequences']
                
                logger.info(f"Invoking model: {model_id}")
                
                response = bedrock_runtime.invoke_model_with_response_stream(
                    modelId=model_id,
                    body=json.dumps(body)
                )
                
                return StreamingHttpResponse(
                    stream_text_prompt_response(response, on_done=on_done_save),
                    content_type='text/event-stream'
                )
            
            # CHAT í…œí”Œë¦¿ ì²˜ë¦¬
            elif template_type == 'CHAT':
                template_config = variant.get('templateConfiguration', {})
                chat_config = template_config.get('chat', {})
                messages = chat_config.get('messages', [])
                system_prompts = chat_config.get('system', [])
                
                logger.info(f"CHAT template - Messages: {len(messages)}, System prompts: {len(system_prompts)}")
                
                inference_config = variant.get('inferenceConfiguration', {})
                
                formatted_messages = []
                for msg in messages:
                    role = msg.get('role', 'user')
                    content_blocks = msg.get('content', [])
                    
                    formatted_content = []
                    for block in content_blocks:
                        if 'text' in block:
                            text = block['text']
                            for var_name, var_value in prompt_variables.items():
                                text = text.replace(f"{{{{{var_name}}}}}", str(var_value))
                            if text.strip():
                                formatted_content.append({"type": "text", "text": text})
                    
                    if formatted_content:
                        content_text = " ".join([c['text'] for c in formatted_content if 'text' in c])
                        if content_text.strip():
                            formatted_messages.append({
                                "role": role,
                                "content": content_text
                            })
                
                # user ë©”ì‹œì§€ê°€ ì—†ê±°ë‚˜ ë§ˆì§€ë§‰ì´ userê°€ ì•„ë‹ˆë©´ ì¶”ê°€
                if not formatted_messages or formatted_messages[-1].get('role') != 'user':
                    formatted_messages.append({
                        "role": "user",
                        "content": user_query
                    })
                elif formatted_messages and not formatted_messages[0].get('content', '').strip():
                    formatted_messages[0]['content'] = user_query
                
                logger.info(f"Formatted {len(formatted_messages)} messages")
                
                body = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": inference_config.get('maxTokens', 4096),
                    "temperature": inference_config.get('temperature', 1.0),
                    "messages": formatted_messages
                }
                
                # System prompt ì²˜ë¦¬
                if system_prompts:
                    system_text = []
                    for sys_prompt in system_prompts:
                        if 'text' in sys_prompt:
                            text = sys_prompt['text']
                            for var_name, var_value in prompt_variables.items():
                                text = text.replace(f"{{{{{var_name}}}}}", str(var_value))
                            system_text.append(text)
                    
                    if system_text:
                        body['system'] = " ".join(system_text)
                
                if 'stopSequences' in inference_config:
                    body['stop_sequences'] = inference_config['stopSequences']
                
                logger.info(f"Invoking model: {model_id}")
                
                response = bedrock_runtime.invoke_model_with_response_stream(
                    modelId=model_id,
                    body=json.dumps(body)
                )
                
                return StreamingHttpResponse(
                    stream_chat_prompt_response(response, on_done=on_done_save),
                    content_type='text/event-stream'
                )
            
            else:
                raise ValueError(f"Unsupported template type: {template_type}")
        
        except bedrock_agent.exceptions.ResourceNotFoundException:
            error_msg = f"Prompt not found: {prompt_id}"
            logger.error(error_msg)
            return StreamingHttpResponse(
                [sse_event({'type': 'error', 'message': error_msg})],
                content_type='text/event-stream'
            )
        
    except Exception as e:
        logger.error(f"Prompt error: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return StreamingHttpResponse(
            [sse_event({'type': 'error', 'message': str(e)})],
            content_type='text/event-stream'
        )

def stream_text_prompt_response(response, on_done=None):
    """TEXT í…œí”Œë¦¿ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
    full_text = ""
    
    try:
        for event in response['body']:
            chunk = json.loads(event['chunk']['bytes'])
            
            if chunk['type'] == 'content_block_delta':
                text = chunk['delta'].get('text', '')
                if text:
                    full_text += text
                    yield sse_event({'type': 'content', 'text': text})
                    logger.info(f"Sent text chunk: {text[:30]}...")
            
            elif chunk['type'] == 'message_stop':
                logger.info(f"Message stop received")
        
        logger.info(f"Stream complete. Total text length: {len(full_text)}")
        
        if callable(on_done):
            on_done(full_text)
        
        yield sse_event({'type': 'done', 'total_length': len(full_text)})
        
    except Exception as e:
        logger.error(f"Streaming error: {str(e)}")
        yield sse_event({'type': 'error', 'message': str(e)})

def stream_chat_prompt_response(response, on_done=None):
    """CHAT í…œí”Œë¦¿ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ë²„í¼ë§)"""
    full_text = ""
    buffer = ""
    buffer_size = 10
    
    try:
        for event in response['body']:
            chunk = json.loads(event['chunk']['bytes'])
            
            if chunk['type'] == 'content_block_delta':
                text = chunk['delta'].get('text', '')
                if text:
                    full_text += text
                    buffer += text
                    if len(buffer) >= buffer_size:
                        yield sse_event({'type': 'content', 'text': buffer})
                        logger.info(f"Sent text chunk: {buffer[:30]}...")
                        buffer = ""
            
            elif chunk['type'] == 'message_stop':
                logger.info(f"Message stop received")
        
        # ë‚¨ì€ ë²„í¼ ì „ì†¡
        if buffer:
            yield sse_event({'type': 'content', 'text': buffer})
            logger.info(f"Sent final buffer: {buffer[:30]}...")
        
        logger.info(f"Stream complete. Total text length: {len(full_text)}")
        
        if callable(on_done):
            on_done(full_text)
        
        yield sse_event({'type': 'done', 'total_length': len(full_text)})
        
    except Exception as e:
        logger.error(f"Streaming error: {str(e)}")
        yield sse_event({'type': 'error', 'message': str(e)})
        
        
# TTS
class TTSSerializer(serializers.Serializer):
    text = serializers.CharField(help_text="bedrockì´ ìƒì„±í•œ ì „ì²´ ë‹µë³€ í…ìŠ¤íŠ¸")     
    promptId = serializers.CharField(help_text="ì¸ë¬¼ì˜ ê³ ìœ  ID (ëª©ì†Œë¦¬ ë§¤í•‘ìš©)")
      
@extend_schema(
    summary="AI ë‹µë³€ TTS ë³€í™˜(Typecast ì‚¬ìš©)",
    request=TTSSerializer,
    description="Typecast APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ í…ìŠ¤íŠ¸ë¥¼ ê³ í’ˆì§ˆ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.",
    responses={200: OpenApiTypes.BINARY},
) 
@csrf_exempt
@api_view(["POST"]) 
def tts_view(request):
    """Bedrockì˜ ìµœì¢… ì‘ë‹µì„ ìŒì„±ìœ¼ë¡œ ë³€í™˜"""
    try:
        logger.info("=" * 50)
        logger.info("ğŸ¤ TTS ìš”ì²­ ì‹œì‘")
        logger.info("=" * 50)
        
        # 1. ìš”ì²­ ë°ì´í„° íŒŒì‹± 
        logger.info("ğŸ“ Step 1: ìš”ì²­ ë°ì´í„° íŒŒì‹± ì¤‘...")
        text = request.data.get('text', '')
        prompt_id = request.data.get('promptId')
        
        logger.info(f"   - text ê¸¸ì´: {len(text)} ë¬¸ì")
        logger.info(f"   - text ë¯¸ë¦¬ë³´ê¸°: {text[:100]}..." if len(text) > 100 else f"   - text: {text}")
        logger.info(f"   - promptId: {prompt_id}")
        
        if not text:
            logger.error("âŒ í…ìŠ¤íŠ¸ê°€ ì œê³µë˜ì§€ ì•ŠìŒ")
            return JsonResponse({'error': 'No text provided'}, status=400)

        # 2. ì¸ë¬¼ ì •ë³´ ì¡°íšŒ
        logger.info("ğŸ‘¤ Step 2: ì¸ë¬¼ ì •ë³´ ì¡°íšŒ ì¤‘...")
        voice_id = None  # ê¸°ë³¸ê°’ ì„¤ì •
        
        if prompt_id:
            try:
                person = AIPerson.objects.get(promptId=prompt_id)
                logger.info(f"   âœ… ì¸ë¬¼ ì°¾ìŒ: {person.name}")
                
                if person.voiceId:
                    voice_id = person.voiceId
                    logger.info(f"   âœ… ëª©ì†Œë¦¬ ID: {voice_id}")
                else:
                    logger.warning(f"   âš ï¸  ì¸ë¬¼ {person.name}ì— voiceIdê°€ ì—†ìŒ")
                    
            except AIPerson.DoesNotExist:
                logger.warning(f"   âš ï¸  promptId {prompt_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.warning("   âš ï¸  promptIdê°€ ì œê³µë˜ì§€ ì•ŠìŒ")

        if not voice_id:
            logger.error("âŒ voice_idë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return JsonResponse({'error': 'voice_id not found'}, status=400)

        # 3. Typecast API ì¤€ë¹„
        logger.info("ğŸ”§ Step 3: Typecast API ì¤€ë¹„ ì¤‘...")
        typecast_api_key = os.getenv('TYPECAST_API_KEY')
        
        if not typecast_api_key:
            logger.error("âŒ TYPECAST_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return JsonResponse({'error': 'TYPECAST_API_KEY not configured'}, status=500)
        
        logger.info(f"   âœ… API í‚¤ í™•ì¸ë¨: {typecast_api_key[:10]}...")
        
        url = "https://api.typecast.ai/v1/text-to-speech"
        logger.info(f"   - API URL: {url}")
        
        headers = {
            "X-API-KEY": typecast_api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "text": text,
            "voice_id": voice_id,
            "language": "ko",
            "model": "ssfm-v21",
            "output": {
                "audio_format": "mp3"
            },
            "options": {
                "pitch": -2
            }
        }
        
        logger.info("   - Payload ìƒì„± ì™„ë£Œ:")
        logger.info(f"     * voice_id: {payload['voice_id']}")
        logger.info(f"     * language: {payload['language']}")
        logger.info(f"     * model: {payload['model']}")
        logger.info(f"     * audio_format: {payload['output']['audio_format']}")
        logger.info(f"     * pitch: {payload['options']['pitch']}")

        # 4. Typecast API í˜¸ì¶œ
        logger.info("ğŸŒ Step 4: Typecast API í˜¸ì¶œ ì¤‘...")
        logger.info(f"   - íƒ€ì„ì•„ì›ƒ: 60ì´ˆ")
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                headers=headers, 
                stream=True, 
                timeout=60
            )
            
            logger.info(f"   âœ… API ì‘ë‹µ ìˆ˜ì‹ : HTTP {response.status_code}")
            logger.info(f"   - Response Headers: {dict(response.headers)}")
            
        except requests.exceptions.Timeout:
            logger.error("âŒ API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ (60ì´ˆ ì´ˆê³¼)")
            return JsonResponse({'error': 'Typecast API timeout'}, status=504)
        except requests.exceptions.ConnectionError as e:
            logger.error(f"âŒ API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            return JsonResponse({'error': 'Cannot connect to Typecast API'}, status=503)
        except Exception as e:
            logger.error(f"âŒ API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return JsonResponse({'error': f'API request failed: {str(e)}'}, status=500)
        
        # 5. ì‘ë‹µ ì²˜ë¦¬
        logger.info("ğŸ“¦ Step 5: ì‘ë‹µ ì²˜ë¦¬ ì¤‘...")
        
        if response.status_code == 200:
            logger.info("   âœ… ìŒì„± ìƒì„± ì„±ê³µ!")
            
            # Content-Length í™•ì¸
            content_length = response.headers.get('Content-Length')
            if content_length:
                logger.info(f"   - ì˜¤ë””ì˜¤ íŒŒì¼ í¬ê¸°: {int(content_length) / 1024:.2f} KB")
            
            res = StreamingHttpResponse(
                response.iter_content(chunk_size=8192), 
                content_type='audio/mpeg'
            )
            res['Content-Disposition'] = f'inline; filename="response_{voice_id}.mp3"'
            
            logger.info(f"   - íŒŒì¼ëª…: response_{voice_id}.mp3")
            logger.info("=" * 50)
            logger.info("âœ… TTS ìš”ì²­ ì™„ë£Œ - ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘")
            logger.info("=" * 50)
            
            return res
            
        else:
            logger.error(f"âŒ API ì‘ë‹µ ì—ëŸ¬: HTTP {response.status_code}")
            
            # ì—ëŸ¬ ì‘ë‹µ ë³¸ë¬¸ í™•ì¸
            try:
                error_body = response.json()
                logger.error(f"   - ì—ëŸ¬ ë‚´ìš©: {error_body}")
            except:
                error_text = response.text[:500]
                logger.error(f"   - ì—ëŸ¬ í…ìŠ¤íŠ¸: {error_text}")
            
            return JsonResponse({
                'error': 'ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì‹¤íŒ¨',
                'status_code': response.status_code,
                'detail': response.text[:200]
            }, status=500)

    except Exception as e:
        logger.error("=" * 50)
        logger.error(f"âŒ TTS ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ")
        logger.error(f"   - ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        logger.error(f"   - ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")
        logger.error("=" * 50)
        
        import traceback
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        
        return JsonResponse({
            'error': str(e),
            'error_type': type(e).__name__
        }, status=500)